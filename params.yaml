# -*- coding: utf-8 -*-

#Parameter for the metric tracking with neptune
neptune:
    project_name: "lukas.eisert/simclr"
    api: "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNzI4NDUzNWQtODdhZi00NjAzLWJkMGYtOGZkYWEzZmJhYzUxIn0="
    tags: ["nnclr", "production", 'with_petro_matching', 'TNG50']

#Parameters for the loading of the data from various sources
extract:
    MIN_STELLAR_MASS: 1e9
    MAX_STELLAR_MASS: 1e14
    DATASETS:
      - "HSC_TNG50"
      - "HSC_TNG100"
      - "HSC"
    SNAPSHOTS:
      - [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
      - [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91]
      - None
    FIELDS: 
      - ["root_descendant_id",
         "lookback", "z",
         "stellar_age_2rhalf_lumw", "fraction_disk_stars",
         "stellar_mass", "mass_in_rad", "mass_exsitu", "half_mass_rad_physical",
         "snap_num_last_maj_merger", "mass_last_maj_merger",
         "mean_merger_lookback_time", "mean_merger_mass_ratio",
         "color", "i_band_mag_dust_apparent", 'asymmetry', 'concentration', 'deviation',
         'gini', 'm20', 'multimode', 'sersic_n', 'sersic_rhalf', 'sersic_ellip', 'smoothness']
      - ["root_descendant_id",
         "lookback", "z",
         "stellar_age_2rhalf_lumw", "fraction_disk_stars",
         "stellar_mass", "mass_in_rad", "mass_exsitu", "half_mass_rad_physical",
         "snap_num_last_maj_merger", "mass_last_maj_merger",
         "mean_merger_lookback_time", "mean_merger_mass_ratio",
         "color", "i_band_mag_dust_apparent", 'asymmetry', 'concentration', 'deviation',
         'gini', 'm20', 'multimode', 'sersic_n', 'sersic_rhalf', 'sersic_ellip', 'smoothness']
      - ["photoz", "petroR90_r", "r_cmodel_mag_ge", "g_cmodel_mag_ge", "i_cmodel_mag_ge", "z_cmodel_mag_ge"]
    #Load datasets?
    LOAD:
      - True
      - True
      - False
    #Only use fractions of the overall set - not 
    FRACTION:
      - 1.0
      - 1.0
      - 0.75
    #Target image size to apply the augmentations to
    IMAGE_SIZE: 256
    #Number of projections per Simualted Image
    NUM_PROJECTIONS: 4
    #Filters to load from the raw images
    FILTERS: ['G', 'R', 'I']
    #Get Cutouts which are multiple of the petrosian radius
    PETRO_RADIUS_FACTOR: 4
    #Use cached data, useful if images are added to a dataset later on or if there was a minor bug while running
    #Set this to False if there are any problems during the loading
    USE_CACHE: True
    #Number of workers to use
    NUM_WORKERS: 8
    #Chunk size of the image loading
    SIZE_CHUNKS: 2000

prepare:
    #Seed for the random split algorithm
    SPLIT_SEED: 42
    #Datasets to use with the split fractions: Training, Validation, Testing
    #Note, that feature scaling is determined by the first set
    SETS:
      - ["HSC", [0.8, 0.0, 0.1, 0.1]]
      - ["HSC_TNG100", [0.8, 0.0, 0.1, 0.1]]
      - ["HSC_TNG50", [0.8, 0.0, 0.1, 0.1]]  
    #Specify fields to match between the sets and the uncertainties they are matched with
    MATCHING_FIELDS:
      - ["photoz", "i_cmodel_mag_ge", 'petro_90_light']
      - ['z', 'i_band_mag_dust_apparent', 'petro_90_light']
      - ['z', 'i_band_mag_dust_apparent', 'petro_90_light']
    MATCHING_UNCERTAINTIES:
    - [0.01, 0.1, 5]
    #The source set is the set drawing samples from to match them to the target set.
    #I.e. we try to find a example in source for each example in the target. 
    MATCHING_SOURCE_SETS: ['HSC']
    MATCHING_TARGET_SETS: ['HSC_TNG100', 'HSC_TNG50']
    #List of observables and unobservables used
    #NOTE: the names might alter from the original dataset_raw fields as prepare.py might modify the fields
    OBSERVABLES: 
      - None
      - None
      - None
    UNOBSERVABLES:
      - ["photoz", "i_cmodel_mag_ge", 'petro_90_light']
      - ['z', 'i_band_mag_dust_apparent', 'petro_90_light']
      - ['z', 'i_band_mag_dust_apparent', 'petro_90_light']
    LABELS: ['z', 'i_band_mag', 'petro_90_radius']
    #Split According to root descendant
    ROOT_DESCENDANT_SPLIT: True
    
data:
    #Valid range for the UNOBSERVABLE parameters; if samples are outside of this interval, results are treated as categorical 
    #Used to identify galaxies with no major mergers
    VALID_RANGE:
      - None
      - None
      - None
    #Choose the Dataset Object which determines how the images are loaded and streched
    DATASET: 'SingleStretchDataset'
    #Target filters to use
    FILTERS: ['I','R','G'] 
    #Target edge size of the images
    IMAGE_SIZE: 128
    #Number of data loading workers
    NUM_WORKERS: 16

params_opt:
    STUDY_NAME: 'Variation_Study'
    STUDY_OBJECT: 'ParameterOptimizationCLR_All'
    NUM_TRIALS: 30
    SAVE_MODELS: True
    SAVE_REPRESENTATIONS: True

train_clr:
    #Learning Rate for the SIMCLR training
    LEARNING_RATE: 0.00015
    #Weight decay for the simclr training
    L2_DECAY: 0.00006
    #Betas for the ADAM optimizer
    BETA_1: 0.9
    BETA_2: 0.999
    #Reduce the Learning rate if the validation loss has not improved for this number of epochs
    LR_PATIENCE: 7
    #Learning rate decay (i.e. the lr is multiplied by the decayfactor every milestone)
    LR_DECAY: 0.6
    #Size of batches used for the training and evaluation
    BATCH_SIZE: 32
    #Maximum number of training epochs
    NUM_EPOCHS: 500
    #If there is a lot of data, one full epoch takes very long. So limit the number of batches in each epoch
    MAX_NUM_BATCHES_PER_EPOCH: 200
    #Stop the training if a max runtime (in seconds) has been reached
    MAX_RUNTIME_SECONDS: 43200
    #Stop the training after validation loss has not improved for PATIENCE epochs
    PATIENCE: 40
    #Train model on multiple devices by splitting and distributing batches 
    #If set to True the device in config.py should be set to the cpu ... torch will take care of the correct assignment
    #Note also that the use of torch.nn.DataParallel is not recommended by Pytorch
    #Updating this might bring a better performance
    #Furthermore there seems to be a problem when loading the parallelized model later on
    PARALLEL_TRAINING: False
    #Train CLR on training and domain set
    DOMAIN_TRAINING: True
    #Domain Adaption during the clr learning
    DOMAIN_ADAPTION: False
    #Contrastive Learning Type (SIMCLR or NNCLR):
    CLR_TYPE: "NNCLR"
    #Size of Queue (i.e. number of batches) in case that NNCLR is used:
    NNCLR_QUEUE_SIZE: 512
    #Augmentation parameters
    AUGMENTATION_PARAMS:
        ROTATION: 15 #In degree
        TRANSLATE: 0.36 #As fraction of the overall image size
        SCALE: 1.9 #max zoom in
        FLIP: True
        GAUSSIAN_BLUR_SIGMA: [0.001, 6.5] #Std in Pixel
        NOISE_STD: [0.01, 0.08] #Noise std (Image values are normalized to 1)
    
train_cinn:
    #Parameters of the Adam Optimizer
    L2_DECAY: 0.0002
    BETA_1: 0.9
    BETA_2: 0.999
    
    #Initial learning rate
    LEARNING_RATE: 0.002
    #Reduce the Learning rate if the validation loss has not improved for this number of epochs
    LR_PATIENCE: 5 
    #Learning rate decay (i.e. the lr is multiplied by the decayfactor every milestone)
    LR_DECAY: 0.5
    
    #Maximum number of training epochs
    NUM_EPOCHS: 200
    #Stop the training after validation loss has not improved for this number of epochs
    PATIENCE: 20

    #Use the simclr pretrained resnet
    USE_PRETRAINED_RESNET: True
    #Fix initialy the Resnet params during training
    FIX_RESNET_PARAMS: True
    #Unfix the RESNET params if the cINN training has reached this learning rate
    #(set to 0 if RESNET should be always fixed)
    RESNET_LR_THRESHOLD: 0.00005
    
    #Size of batches used for the training (and validation)
    BATCH_SIZE: 128

    #Level of the gaussian noise augmentation (Note that quantities are normalized)
    NOISE: 0.0
    #Number of models used in the averaging
    NUM_MODELS: 1
    
    #Augmentation parameters
    AUGMENTATION_PARAMS:
        ROTATION: 10
        TRANSLATE: 0.15
        SCALE: 1.2
        FLIP: True
        GAUSSIAN_BLUR_SIGMA: [0.1, 5]
        NOISE_STD: [0.0, 0.05]

model:
    #Clamp parameter of the coupling layers
    CLAMP: 1.0
    
    #Number of coupling layers
    NUM_COUPLING_LAYERS: 12
    
    #Conditional input size of the coupling layers 
    NUM_COND_NODES: 128
    
    #Width and Depth of the hidden networks in the coupling layers 
    NUM_HIDDEN_NODES: 128
    NUM_HIDDEN_LAYERS: 2
    #Dropout and Batch Norm between the layers in the hidden networks in the coupling layers
    DROPOUT: 0.0
    BATCH_NORM: False

    #Width and Depth of the conditional network
    NUM_HIDDEN_NODES_COND: 128    
    NUM_HIDDEN_LAYERS_COND: 2
    #Dropout and Batch Norm between the layers in the conditional network
    DROPOUT_COND: 0.0
    BATCH_NORM_COND: False

    #Width and Depth of the MLP network
    NUM_HIDDEN_NODES_MLP: 128    
    NUM_HIDDEN_LAYERS_MLP: 2
    #Dropout and Batch Norm between the layers in the MLP network
    DROPOUT_MLP: 0.0
    BATCH_NORM_MLP: False
    
    #Parameters for the resnet 
    #Resnet depth should be 6n+4
    RESNET_DEPTH: 16
    RESNET_WIDTH: 2
    RESNET_DROPOUT: 0.3
    RESNET_REPRESENTATION_DIM: 128
    RESNET_REPRESENTATION_DEPTH: 3
    RESNET_PROJECTION_DIM: 64
    RESNET_PROJECTION_DEPTH: 2
    RESNET_NUM_CHANNELS: 3
    
    #The Number of discrete rotations the RESNET is initially equivariant
    RESNET_ROTATION_EQUIVARIANCE: 8
    #Restrict the Rotation equivariance 
    #0 = No restriction
    #1 = Restriction before the last block to N/2
    #2 = Restriction after the first block to N/2
    #3 = Restriction after the first (to N/2) and the second block (to 1)
    RESNET_ROTATION_RESTRICTION: 0
    #Flag if the RESNET should be reflection invariant
    RESNET_REFLECTION_EQUIVARIANCE: True
    #Initial Stride of the Resnet
    RESNET_INITIAL_STRIDE: 2
    
    #NOTE: If RESNET_ROTATION_EQUIVARIANCE = 1 and RESNET_REFLECTION_EQUIVARIANCE = False
    #the RESNET should be equivalent to a standard CNN

losses:
    #Kernel vor forward and backward maximum mean discrepancy loss
    mmd_kernel_type: "inverse_multiquadratic"
    mmd_kernels: [[0.2, 0.1], [0.2, 0.5], [0.2, 2]]
    
    #Loss type for the adaption
    adaption_type: "huber_linear_mmd"
    
    #Prefactor for the contrastive learning losses
    lambd_clr_train: 1.
    lambd_clr_domain: 0.
    lambd_clr_adaption: 0.
    
    #Prefactor for the cINN losses
    #i.e. the overall loss is given as weighted sum with the weights given as prefactors
    lambd_max_likelihood: 1.
    lambd_mmd_forw: 0.
    lambd_mmd_back: 0.
    lambd_mae: 0.
    lambd_mse: 0.
    
    #Softmax temperature for SimCLR NCE Loss
    nce_temperature: 0.07
    
sample_posterior:
    #Number of posterior samples drawn for each galaxy in the test set
    NUM_SAMPLES: 400

peak_detection:
    EVAL_BINS: 512
    MIN_PEAK_DISTANCE: 32
    MIN_PEAK_PROMINENCE: 0.02
    MIN_PEAK_HEIGHT: 0.05
