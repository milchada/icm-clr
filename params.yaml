# -*- coding: utf-8 -*-

#Parameter for the metric tracking with neptune
neptune:
    project_name: "lukas.eisert/simclr"
    api: "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNzI4NDUzNWQtODdhZi00NjAzLWJkMGYtOGZkYWEzZmJhYzUxIn0="
    tags: ["debug"]

#Mass, Snapshots and Simulations to get the data from
extract:
    MIN_STELLAR_MASS: 1e10
    MAX_STELLAR_MASS: 1e12
    SNAPSHOTS: [72, 78, 84, 91]
    SIMULATIONS:
      - "TNG50-1"

prepare:
    #Seed for the random split algorithm
    SPLIT_SEED: 0
    #Simulation to use with the split fractions: Training, Validation, Testing
    #Note, that feature scaling is determined by the first set
    SETS:
      - ["TNG50-1", [0.8, 0.1, 0.1]]
    #List of observables and unobservables used
    #NOTE: the names might alter from the original dataset_raw fields as prepare.py is modifying the fields
    OBSEVABLES:
      - 'mass'
      - 'lookback'
    UNOBSERVABLES:
      - 'lookback'
      - 'mass'
      - 'fraction_disk_stars'
      - 'stellar_age'
      - 'mass_last_maj_merger'
      - 'exsitu'
    #Split According to root descendant
    ROOT_DESCENDANT_SPLIT: True
    
data:
    #Valid range for the UNOBSERVABLE parameters; if samples are outside of this interval, results are treated as categorical 
    #Used to identify galaxies with no major mergers
    VALID_RANGE:
      - None
      - None
      - None
      - None
      - [7., 11.5]
      #- [0., 13.7]
      - None
    #Edge size of the images
    IMAGE_SIZE: 128
    #Number of data loading workers
    NUM_WORKERS: 18

train_simclr:
    #Learning Rate for the SIMCLR training
    LEARNING_RATE: 0.0003
    #Weight decay for the simclr training
    WEIGHT_DECAY: 0.0001
    #Size of batches used for the training and evaluation
    BATCH_SIZE: 64
    #Maximum number of training epochs
    NUM_EPOCHS: 40
    #Stop the training after validation loss has not improved for PATIENCE epochs
    PATIENCE: 5

train_cinn:
    #Parameters of the Adam Optimizer
    L2_DECAY: 0.0001
    BETA_1: 0.9
    BETA_2: 0.999
    
    #Initial learning rate
    LEARNING_RATE: 0.002
    #Reduce the Learning rate if the validation loss has not improved for this number of epochs
    LR_PATIENCE: 5 
    #Learning rate decay (i.e. the lr is multiplied by the decayfactor every milestone)
    LR_DECAY: 0.5
    
    #Maximum number of training epochs
    NUM_EPOCHS: 200
    #Stop the training after validation loss has not improved for this number of epochs
    PATIENCE: 20

    #Use the simclr pretrained resnet
    USE_PRETRAINED_RESNET: True
    #Fix initialy the Resnet params during training
    FIX_RESNET_PARAMS: True
    #Unfix the RESNET params if the cINN training has reached this learning rate
    #(set to 0 if RESNET should be always fixed)
    RESNET_LR_THRESHOLD: 0.00005
    
    #Size of batches used for the training (and validation)
    BATCH_SIZE: 256

    #Level of the gaussian noise augmentation (Note that quantities are normalized)
    NOISE: 0.025
    #Number of models used in the averaging
    NUM_MODELS: 1

model:
    #Clamp parameter of the coupling layers
    CLAMP: 1.0
    
    #Number of coupling layers
    NUM_COUPLING_LAYERS: 12
    
    #Conditional input size of the coupling layers 
    NUM_COND_NODES: 128
    
    #Width and Depth of the hidden networks in the coupling layers 
    NUM_HIDDEN_NODES: 128
    NUM_HIDDEN_LAYERS: 2
    
    #Dropout and Batch Norm between the layers in the hidden networks in the coupling layers
    DROPOUT: 0.0
    BATCH_NORM: False

    #Width and Depth of the conditional network
    NUM_HIDDEN_NODES_COND: 128    
    NUM_HIDDEN_LAYERS_COND: 2
    
    #Dropout and Batch Norm between the layers in the conditional network
    DROPOUT_COND: 0.0
    BATCH_NORM_COND: False
    
    #Parameters for the resnet 
    RESNET_DEPTH: 16
    RESNET_WIDTH: 4
    RESNET_DROPOUT: 0.3
    RESNET_REPRESENTATION_DIM: 128
    RESNET_PROJECTION_DIM: 128
    RESNET_PROJECTION_DEPTH: 2
    RESNET_NUM_CHANNELS: 3
    
    #The Number of discrete rotations the RESNET is initially equivariant
    RESNET_ROTATION_EQUIVARIANCE: 8
    #Restrict the Rotation equivariance 
    #0 = No restriction
    #1 = Restriction before the last block to N/2
    #2 = Restriction after the first block to N/2
    #3 = Restriction after the first (to N/2) and the second block (to 1)
    RESNET_ROTATION_RESTRICTION: 0
    #Flag if the RESNET
    RESNET_REFLECTION_EQUIVARIANCE: True
    #Initial Stride of the Resnet
    RESNET_INITIAL_STRIDE: 2
    
    #NOTE: If RESNET_ROTATION_EQUIVARIANCE = 1 and RESNET_REFLECTION_EQUIVARIANCE = False
    #the RESNET should be equivalent to standard CNNs

losses:
    #Kernel vor forward and backward maximum mean discrepancy loss
    mmd_forw_kernels: [[0.2, 2], [1.5, 2], [3.0, 2]]
    mmd_back_kernels: [[0.2, 0.1], [0.2, 0.5], [0.2, 2]]
    
    #Prefactor for the cINN losses
    #i.e. the overall loss is given as weighted sum with the weights given as prefactors
    lambd_max_likelihood: 1.
    lambd_mmd_forw: 0.
    lambd_mmd_back: 0.
    lambd_mae: 0.
    lambd_mse: 0.
    
    #Softmax temperature for SimCLR NCE Loss
    nce_temperature: 0.07
    
sample_posterior:
    #Number of posterior samples drawn for each galaxy in the test set
    NUM_SAMPLES: 400

peak_detection:
    EVAL_BINS: 512
    MIN_PEAK_DISTANCE: 32
    MIN_PEAK_PROMINENCE: 0.02
    MIN_PEAK_HEIGHT: 0.05